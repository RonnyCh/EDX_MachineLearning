---
title: "Harvardx Machine Learning"
output: html_notebook
---

# Comprehension Check: Cross-validation

# Q1
Generate a set of random predictors and outcomes using the following code:

```{r}
library(dplyr)
set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()

x_subset <- x[ ,sample(p, 100)]
```



Because x and y are completely independent, you should not be able to predict y using x with accuracy greater than 0.5. Confirm this by running cross-validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model.

Which code correctly performs this cross-validation?

```{r}
library(caret)
fit <- train(x_subset, y, method = "glm")
fit$results 
```

# Q2

Now, instead of using a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the 
y=1 group to those in the y=0 group, for each predictor, using a t-test. You can do perform this step like this:

Load the following codes to answer question 2. This came from someone in discussion forum
```{r}
install.packages("BiocManager")
BiocManager::install("genefilter",version = "3.8")

library(genefilter)
tt <- colttests(x, y)
```

Which of the following lines of code correctly creates a vector of the p-values called pvals?

Answers : pvals <- tt$p.value

# Q3
Create an index ind with the column numbers of the predictors that were "statistically significantly" associated with y. Use a p-value cutoff of 0.01 to define "statistically significantly."

How many predictors survive this cutoff?

```{r}
ind <- which(pvals<=0.01)
length(ind)
```

# Q4
Now re-run the cross-validation after redefinining x_subset to be the subset of x defined by the columns showing "statistically significant" association with y.

What is the accuracy now?

```{r}
#x_subset <- x[ ,sample(p, 100)]
x_subset <- x[,ind]
fit <- train(x_subset, y, method = "glm")
fit
```

# Q5
Re-run the cross-validation again, but this time using kNN. Try out the following grid k = seq(101, 301, 25) of tuning parameters. Make a plot of the resulting accuracies.

Which code is correct?
```{r}
fit <- train(x_subset, y, method = "knn", tuneGrid = data.frame(k = seq(101, 301, 25)))
ggplot(fit)
```


```{r}
library(dslabs)
data(tissue_gene_expression)
str(tissue_gene_expression)

fit <- train(y~x, method = "knn",data=tissue_gene_expression)
ggplot(fit,highlight=TRUE)
```


seq(1,50,2)

# Comprehension Check: Bootstrap

# Q1
The createResample function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the mnist_27 dataset like this:

```{r}
library(dslabs)
library(caret)
data(mnist_27)
set.seed(1995)
indexes <- createResample(mnist_27$train$y, 10)
```

Enter the numbers of 3,4 and 7 appear in indexes

```{r}
sum(indexes[[1]] == 3)
sum(indexes[[1]] == 4)
sum(indexes[[1]] == 7)
```


# Q2

We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the resampled indexes.

What is the total number of times that 3 appears in all of the resampled indexes?

```{r}
x=sapply(indexes, function(ind){
	sum(ind == 3)
})
sum(x)
```

# Q3

Generate a random dataset using the following code:
```{r}
set.seed(1)
y <- rnorm(100, 0, 1)
```

Estimate the 75th quantile, which we know is qnorm(0.75), with the sample quantile: quantile(y, 0.75).

Run a Monte Carlo simulation with 10,000 repetitions to learn the expected value and standard error of this random variable. Set the seed to 1.

```{r}
set.seed(1)
B <- 10000
q_75 <- replicate(B, {
	y <- rnorm(100, 0, 1)
	quantile(y, 0.75)
})

mean(q_75)
sd(q_75)
```

# Q4
In practice, we can't run a Monte Carlo simulation. Use 10 bootstrap samples to estimate the standard error using just the initial sample y. Set the seed to 1.

```{r}
set.seed(1)
indexes <- createResample(y, 10)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```
# Q5
Repeat the exercise from Q4 but with 10,000 bootstrap samples instead of 10. Set the seed to 1.

```{r}
set.seed(1)
indexes <- createResample(y, 10000)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```


