---
title: "Harvardx Machine Learning"
output: html_notebook
---

# Comprehension Check: Cross-validation

# Q1
Generate a set of random predictors and outcomes using the following code:

```{r}
library(dplyr)
set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()

x_subset <- x[ ,sample(p, 100)]
```



Because x and y are completely independent, you should not be able to predict y using x with accuracy greater than 0.5. Confirm this by running cross-validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model.

Which code correctly performs this cross-validation?

```{r}
library(caret)
fit <- train(x_subset, y, method = "glm")
fit$results 
```

# Q2

Now, instead of using a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the 
y=1 group to those in the y=0 group, for each predictor, using a t-test. You can do perform this step like this:

Load the following codes to answer question 2. This came from someone in discussion forum
```{r}
install.packages("BiocManager")
BiocManager::install("genefilter",version = "3.8")

library(genefilter)
tt <- colttests(x, y)
```

Which of the following lines of code correctly creates a vector of the p-values called pvals?

Answers : pvals <- tt$p.value

# Q3
Create an index ind with the column numbers of the predictors that were "statistically significantly" associated with y. Use a p-value cutoff of 0.01 to define "statistically significantly."

How many predictors survive this cutoff?

```{r}
ind <- which(pvals<=0.01)
length(ind)
```

# Q4
Now re-run the cross-validation after redefinining x_subset to be the subset of x defined by the columns showing "statistically significant" association with y.

What is the accuracy now?

```{r}
#x_subset <- x[ ,sample(p, 100)]
x_subset <- x[,ind]
fit <- train(x_subset, y, method = "glm")
fit
```

# Q5
Re-run the cross-validation again, but this time using kNN. Try out the following grid k = seq(101, 301, 25) of tuning parameters. Make a plot of the resulting accuracies.

Which code is correct?
```{r}
fit <- train(x_subset, y, method = "knn", tuneGrid = data.frame(k = seq(101, 301, 25)))
ggplot(fit)
```


```{r}
library(dslabs)
data(tissue_gene_expression)
str(tissue_gene_expression)

fit <- train(y~x, method = "knn",data=tissue_gene_expression)
ggplot(fit,highlight=TRUE)
```


seq(1,50,2)

# Comprehension Check: Bootstrap

# Q1
The createResample function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the mnist_27 dataset like this:

```{r}
library(dslabs)
library(caret)
data(mnist_27)
set.seed(1995)
indexes <- createResample(mnist_27$train$y, 10)
```

Enter the numbers of 3,4 and 7 appear in indexes

```{r}
sum(indexes[[1]] == 3)
sum(indexes[[1]] == 4)
sum(indexes[[1]] == 7)
```


# Q2

We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the resampled indexes.

What is the total number of times that 3 appears in all of the resampled indexes?

```{r}
x=sapply(indexes, function(ind){
	sum(ind == 3)
})
sum(x)
```

# Q3

Generate a random dataset using the following code:
```{r}
set.seed(1)
y <- rnorm(100, 0, 1)
```

Estimate the 75th quantile, which we know is qnorm(0.75), with the sample quantile: quantile(y, 0.75).

Run a Monte Carlo simulation with 10,000 repetitions to learn the expected value and standard error of this random variable. Set the seed to 1.

```{r}
set.seed(1)
B <- 10000
q_75 <- replicate(B, {
	y <- rnorm(100, 0, 1)
	quantile(y, 0.75)
})

mean(q_75)
sd(q_75)
```

# Q4
In practice, we can't run a Monte Carlo simulation. Use 10 bootstrap samples to estimate the standard error using just the initial sample y. Set the seed to 1.

```{r}
set.seed(1)
indexes <- createResample(y, 10)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```
# Q5
Repeat the exercise from Q4 but with 10,000 bootstrap samples instead of 10. Set the seed to 1.

```{r}
set.seed(1)
indexes <- createResample(y, 10000)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```




# Notes from Rafalab regarding generative model

Let’s start with a very simple and uninteresting, yet illustrative, case: the example related to predicting sex from height.

```{r}
library(caret)
library(dslabs)
data("heights")
y <- heights$height
set.seed(2)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)
```

Using the predictor under the assumption it is normal distribution

```{r}
params <- train_set %>% 
  group_by(sex) %>% 
  summarize(avg = mean(height), sd = sd(height))
params
```

The prevalence, which we will denote with  
π=Pr(Y=1), can be estimated from the data with:


```{r}
pi <- train_set %>% 
  summarize(pi=mean(sex=="Female")) %>% 
  .$pi
pi
```

Now we can use our estimates of average and standard deviation to get an actual rule:

```{r}
x <- test_set$height

f0 <- dnorm(x, params$avg[2], params$sd[2])
f1 <- dnorm(x, params$avg[1], params$sd[1])

p_hat_bayes <- f1*pi / (f1*pi + f0*(1 - pi))
plot(x,p_hat_bayes)
```


# Controlling prevalence
As we discussed earlier, our sample has a much lower prevalence, 0.229, than the general population. So if we use the rule  p(x)>0.5 to predict females, our accuracy will be affected due to the low sensitivity:


```{r}
y_hat_bayes <- ifelse(p_hat_bayes > 0.5, "Female", "Male")
sensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))
```

Again, this is because the algorithm gives more weight to specificity to account for the low prevalence:

```{r}
specificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))
```

This is due mainly to the fact that π is substantially less than 0.5, so we tend to predict Male more often. It makes sense for a machine learning algorithm to do this in our sample because we do have a higher percentage of males. But if we were to extrapolate this to a general population, our overall accuracy would be affected by the low sensitivity.

The Naive Bayes approach gives us a direct way to correct this since we can simply force  
pi to be, for example,  π. So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule, we could simply change  π:


```{r}
p_hat_bayes_unbiased <- f1*0.5 / (f1*0.5 + f0*(1-0.5)) 
y_hat_bayes_unbiased <- ifelse(p_hat_bayes_unbiased> 0.5, "Female", "Male")


sensitivity(data = factor(y_hat_bayes_unbiased), reference = factor(test_set$sex))
specificity(data = factor(y_hat_bayes_unbiased), reference = factor(test_set$sex))

```

Note the difference in sensitivity with a better balance:

The new rule also gives us a very intuitive cutoff between 66-67, which is about the middle of the female and male average heights:

```{r}
qplot(x, p_hat_bayes_unbiased, geom = "line") + 
  geom_hline(yintercept = 0.5, lty = 2) + 
  geom_vline(xintercept = 67, lty = 2)
```

